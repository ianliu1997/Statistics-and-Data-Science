{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab2.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:10.462208Z",
     "iopub.status.busy": "2023-09-14T03:44:10.461817Z",
     "iopub.status.idle": "2023-09-14T03:44:11.123688Z",
     "shell.execute_reply": "2023-09-14T03:44:11.122634Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1><center>SDSE Lab 2 <br>Solving optimization problems with <br> Gradient Descent and <br> Stochastic Gradient Descent </center></h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "**Part I**:  Estimating $\\lambda$ for an exponential distribution\n",
    "+ I-1) Sampling a dataset\n",
    "+ I-2) Cost function\n",
    "+ I-3) Analytical solution\n",
    "\t+ I-3.1) Compute $\\lambda_*$\n",
    "\t+ I-3.2) Plot\n",
    "\t+ I-3.3) Variations due to uncertainty in the data\n",
    "\t\t+ I-3.3.1) Variations in $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and $\\lambda_*$\n",
    "\t\t+ I-3.3.2) Plot $\\mathcal{J}_1(\\lambda;\\mathcal{D})$\n",
    "\t\t+ I-3.3.3) Histogram of $\\lambda_*$\n",
    "+ I-4) Gradient descent\n",
    "\t+ I-4.1) Compute $\\mathcal{J}_1'(\\lambda,\\mathcal{D})$\n",
    "\t+ I-4.2) Plot $\\mathcal{J}_1'$\n",
    "\t+ I-4.3) Code gradient descent\n",
    "\t+ I-4.4) Plot gradient descent\n",
    "\t+ I-4.5) Count evaluations of $\\mathcal{J}_1'(\\lambda;\\mathcal{D})$\n",
    "+ I-5) Grid search\n",
    "\t+ I-5.1) Code grid search\n",
    "\t+ I-5.2) Count evaluations of $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ \n",
    "\t+ I-5.3) Plot grid search\n",
    "\n",
    "**Part II**: Estimating $\\mu$ and $\\sigma^2$ for a normal distribution\n",
    " \n",
    "+ II-1) Cost function\n",
    "+ II-2) Analytical solution\n",
    "\t+ II-2.1) Compute $\\mu_*$ and $\\sigma^2_*$\n",
    "+ II-3) Gradient descent\n",
    "\t+ II-3.1) Compute the gradient\n",
    "\t+ II-3.2) 2D gradient descent\n",
    "\t+ II-3.3) Plot gradient descent\n",
    "\t+ II-3.4) Run this GD over a grid of initial conditions.\n",
    "+ II-4) Stochastic gradient descent\n",
    "\t+ II-4.1) Code stochastic gradient descent\n",
    "\t+ II-4.2) Plot stochastic gradient descent\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this lab exercise we use gradient descent (GD) and stochastic gradient descent (SGD) to solve the maximum likelihood problem, which is an important problem in statistics. The details of maximum likelihood will be covered in the lecture on point estimation. Here we take the problem statement as granted and concern ourselves only with solving it.\n",
    "\n",
    "# Problem statement\n",
    "\n",
    "Suppose that we have a system that produces real-valued (scalar) measurements. The system is modeled as a random variable $Y$ with an unknown pdf $p_Y$. The dataset $\\mathcal{D}$ consists of $N$ measurements iid sampled from $Y$.\n",
    "\\begin{equation*}\n",
    "\\mathcal{D} = \\{ y_i \\}_N \\sim Y\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal is to use $\\mathcal{D}$ to estimate a distribution $p$ that best approximates $p_Y$. Here we use the *maximum likelihood* technique, which solves the problem in two steps. The first step is to guess which family of distributions best fits $Y$. This can be done by looking at a histogram of the data, or based on expert opinion. \n",
    "The second step, given our choice, is to solve an optimization problem for the optimal values of the parameters. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\underset{\\theta_1,...\\theta_D}{\\text{minimize}}\\;  -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p(y_i;\\theta_1,...\\theta_D)  \n",
    "\\end{equation*}\n",
    "\n",
    "Here, $p(y_i;\\theta_1,...\\theta_D)$ is the pdf of the selected family with parameters $\\theta_1,...,\\theta_D$, evaluated on the sample $y_i\\in\\mathcal{D}$. It is not important for this lab to understand the reasoning behind this formula. For now we regard it simply as a function to be minimized. We refer to this function as the *cost function* $\\mathcal{J}(\\theta_1,...\\theta_D;\\mathcal{D})$. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{J}(\\theta_1,...\\theta_D;\\mathcal{D}) = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p(y_i;\\theta_1,...\\theta_D)  \n",
    "\\end{equation*}\n",
    "\n",
    "This notation emphasize that we minimize $\\mathcal{J}$ over the parameters $\\theta_1,...,\\theta_D$ with the dataset $\\mathcal{D}$ held fixed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part I:**  Estimating $\\lambda$ for an exponential distribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I-1) Sampling a dataset\n",
    "\n",
    "The method `draw_50_samples_from_I` returns an array of 50 floating point values that represent 50 iid measurements from an input-less system. We pretend that we do not know the underlying process, but in fact the data was generated from an exponential distribution with $\\lambda=10$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:11.128535Z",
     "iopub.status.busy": "2023-09-14T03:44:11.128112Z",
     "iopub.status.idle": "2023-09-14T03:44:11.133714Z",
     "shell.execute_reply": "2023-09-14T03:44:11.132878Z"
    }
   },
   "outputs": [],
   "source": [
    "D = draw_50_samples_from_I()\n",
    "N = len(D)\n",
    "\n",
    "print(f\"The dataset has {N} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data. The top row in the figure below shows the data points arranged on a line. The bottom row shows a histogram of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:11.190671Z",
     "iopub.status.busy": "2023-09-14T03:44:11.190273Z",
     "iopub.status.idle": "2023-09-14T03:44:11.644536Z",
     "shell.execute_reply": "2023-09-14T03:44:11.643242Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5),nrows=2,sharex=True)\n",
    "ax[0].plot(D,np.zeros(N),'r.')\n",
    "ax[1].hist(D);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram, and perhaps based on our understanding of the system, we postulate that $p_Y$ is exponential. \n",
    "\n",
    "\\begin{equation*}\n",
    "p(y;\\lambda) = \\lambda e^{-\\lambda y}\n",
    "\\end{equation*}\n",
    "$\\lambda$ is unknown. We plug this formula into the expression for the cost function:\n",
    "\\begin{align*}\n",
    "\\mathcal{J}_1(\\lambda;\\mathcal{D}) &= -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p(y_i;\\lambda)   \\\\\n",
    " &= -\\frac{1}{N}\\sum_{i=1}^{N} \\ln\\left( \\lambda e^{-\\lambda y} \\right)  \\\\\n",
    "&=  - \\ln\\lambda +  \\frac{\\lambda}{N}\\sum_{i=1}^N y_i\n",
    "\\end{align*}\n",
    "We call this $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ to distinguish it from the cost function in part 2 of this lab. \n",
    "The expression is simplified by defining $\\bar{y}$ as the mean of the samples in $\\mathcal{D}$.\n",
    "\\begin{equation*}\n",
    "\\mathcal{J}_1(\\lambda;\\mathcal{D}) =  - \\ln\\lambda + \\lambda \\: \\bar{y}\n",
    "\\end{equation*}\n",
    "\n",
    "Next we will write code to find the value of $\\lambda_*$ that minimizes $\\mathcal{J}_1(\\lambda;\\mathcal{D})$. We will do this by both analytical and numerical means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-2) Cost function\n",
    "\n",
    "Write a function that computes $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ for a given $\\lambda$ and dataset $\\mathcal{D}$. (This can be done with one line of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:11.649102Z",
     "iopub.status.busy": "2023-09-14T03:44:11.648789Z",
     "iopub.status.idle": "2023-09-14T03:44:11.656631Z",
     "shell.execute_reply": "2023-09-14T03:44:11.655580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def curlyJ1(lmbda,D):\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-3) Analytical solution\n",
    "\n",
    "The first order optimality conditions for this problem tell us that, if a solution $\\lambda_*$ exists, then it must be a *stationary point* of $\\mathcal{J}_1(\\lambda;\\mathcal{D})$, meaning one where the derivative is zero. That is, if $\\lambda_*$ is a solution, then\n",
    "\\begin{equation*}\n",
    "\\mathcal{J}_1'(\\lambda_*;\\mathcal{D}) = 0\n",
    "\\end{equation*}\n",
    "This does not mean that all stationary points are solutions. Some stationary points may be maxima instead of minima. Some may be local minima but not global minima. But if there is a small number of stationary points, then we can evaluate $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ on each one, and be assured that the one with the lowest value is $\\lambda_*$. \n",
    "\n",
    "Our strategy for finding $\\lambda_*$ therefore begins with finding all of the stationary points. That is, all of the values $\\lambda_s$ that satisfy $\\mathcal{J}_1'(\\lambda_s,\\mathcal{D}) = 0$. Solving this equation produces a *unique* stationary point:\n",
    "\\begin{equation*}\n",
    "\\lambda_s = 1/\\bar{y}\n",
    "\\end{equation*}\n",
    "\n",
    "Since there is only one (and verifying that it is a local minimum) we conclude that it solves the optimization problem.\n",
    "\\begin{equation*}\n",
    "\\lambda_* = 1/\\bar{y}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3.1) Compute $\\lambda_*$\n",
    "\n",
    "Write a function that computes $\\lambda_*$ for a given $\\mathcal{D}$. (This can be done in one line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:11.680930Z",
     "iopub.status.busy": "2023-09-14T03:44:11.680632Z",
     "iopub.status.idle": "2023-09-14T03:44:11.686829Z",
     "shell.execute_reply": "2023-09-14T03:44:11.686040Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_lambda_star(D):\n",
    "    return ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3.2) Plot\n",
    "\n",
    "Plot the cost function $\\mathcal{J}_1$ evaluated on logarithmically spaced points (numpy `logspace`) ranging from $10^{-2}$ to $10^{-0.5}$. Place a vertical line (matplotlib `axvline`) at $\\lambda_*$.\n",
    "\n",
    "**Hint**: Here is a nice way of evaluating a function `f` on a grid of points `G` using a list comprehension:\n",
    "\n",
    "```python\n",
    "values = np.array([f(g) for g in G])\n",
    "```\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"I_2_2_2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:11.700234Z",
     "iopub.status.busy": "2023-09-14T03:44:11.699947Z",
     "iopub.status.idle": "2023-09-14T03:44:11.957678Z",
     "shell.execute_reply": "2023-09-14T03:44:11.956661Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmbdas = ...   # TODO\n",
    "J1s = ...  # TODO\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(...,...,'.-')  # TODO\n",
    "ax.axvline(...,color='r',linestyle='--')  # TODO\n",
    "ax.grid(linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3.3) Variations due to uncertainty in the data\n",
    "\n",
    "The computation of both the cost function $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and the optimal value $\\lambda_*$ were based on the randomly sampled dataset $\\mathcal{D}$. Next we will see the effect that variations in $\\mathcal{D}$ have on $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and $\\lambda_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I-3.3.1) Variations in $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and $\\lambda_*$\n",
    "\n",
    "Compute $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ and $\\lambda_*$ for 300 independently samples datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:11.962321Z",
     "iopub.status.busy": "2023-09-14T03:44:11.961975Z",
     "iopub.status.idle": "2023-09-14T03:44:12.115872Z",
     "shell.execute_reply": "2023-09-14T03:44:12.114835Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_datasets = 300\n",
    "Ds = draw_50_samples_from_I_300_times()\n",
    "J1_samples = np.empty((num_datasets,lmbdas.shape[0]))\n",
    "lambda_star_samples = np.empty(num_datasets)\n",
    "for i in range(num_datasets):\n",
    "    D = Ds[i,:]\n",
    "    J1_samples[i,:] = ...  # TODO\n",
    "    lambda_star_samples[i] = ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_3_3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I-3.3.2) Plot $\\mathcal{J}_1(\\lambda;\\mathcal{D})$\n",
    "\n",
    "Plot the 300 versions of $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ on a single plot. Pass parameters `c='k'`,`linewidth=0.5`, and `alpha=0.1` to the `plot` function.\n",
    "\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"I_2_2_3_2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:12.151298Z",
     "iopub.status.busy": "2023-09-14T03:44:12.150993Z",
     "iopub.status.idle": "2023-09-14T03:44:12.777603Z",
     "shell.execute_reply": "2023-09-14T03:44:12.776676Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(...)  # TODO\n",
    "ax.grid(linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I-3.3.3) Histogram of $\\lambda_*$\n",
    "\n",
    "Make a histogram of the 300 samples of $\\lambda_*$. The histogram should have 50 bins. Draw a vertical line on it indicating the average of the values.\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"I_2_2_3_3.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:12.782111Z",
     "iopub.status.busy": "2023-09-14T03:44:12.781677Z",
     "iopub.status.idle": "2023-09-14T03:44:13.109989Z",
     "shell.execute_reply": "2023-09-14T03:44:13.108911Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(...)  # TODO\n",
    "ax.axvline(...)  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving optimization problems is usually not this easy to do by hand. More often we must use a numerical method executed on a computer. There are many algorithms for doing this, and they divide roughly into two types: ones that use gradient information and ones that do not. Here we will demonstrate canonical examples of each of these types. \n",
    "\n",
    "## I-4 Gradient descent\n",
    "\n",
    "Gradient-based algorithms assume that we have at our disposal a method for evaluating the derivative (a.k.a. the gradient) of the cost function. Given this requirement, the procedure begins by making a guess $\\lambda_0$. We call $\\lambda_0$ the *initial condition* of the algorithm. If $\\lambda_0$ happens to be a stationary point (ie. $\\mathcal{J}_1'(\\lambda_0;\\mathcal{D})=0$), then the procedure terminates and returns $\\lambda_0$. Otherwise it proceeds to compute $\\lambda_1$ with\n",
    "\\begin{equation*}\n",
    "\\lambda_{1} = \\lambda_{0} - \\gamma \\: \\mathcal{J}_1'(\\lambda_0;\\mathcal{D}) \n",
    "\\end{equation*}\n",
    "Here $\\gamma$ is a positive number called the *step size*. The reasoning behind this formula is that by taking steps in the direction of the *negative gradient*, the algorithm will eventually reach a local minimum. This is true so long as $\\gamma$ is chosen correctly (not too big). The general formula for gradient descent is,\n",
    "\\begin{equation*}\n",
    "\\lambda_{k+1} = \\lambda_{k} - \\gamma \\: \\mathcal{J}_1'(\\lambda_k;\\mathcal{D}) \\qquad k\\in\\{0,...,K-1\\}\n",
    "\\end{equation*}\n",
    "Here $K$ is the number of steps taken.\n",
    "There are a couple enhancements to consider. \n",
    "\n",
    "1) We can stop the process once we are within a tolerance value of a stationary point. That is, if the gradient becomes less than a pre-determined value $\\tau$:\n",
    "\\begin{equation*}\n",
    "|\\mathcal{J}_1'(\\lambda_k;\\mathcal{D})|<\\tau\n",
    "\\end{equation*}\n",
    "which is equivalent to \n",
    "\\begin{equation*}\n",
    "|\\lambda_{k+1}-\\lambda_k|<\\gamma\\tau\n",
    "\\end{equation*}\n",
    "\n",
    "2) For our particular problem, it is important to ensure that $\\lambda_{k+1}$ does not become negative, since $\\mathcal{J}_1'(\\lambda_{k+1};\\mathcal{D})$ would not be defined. To prevent this we can use $\\lambda_{k+1} =\\lambda_k/2$ whenever the standard formula produces a negative value.\n",
    "\n",
    "In this lab exercise we will implement the simple version of gradient descent *without these two enhancements*. \n",
    "\n",
    "We will need a function that computes the gradient of $\\mathcal{J}_1$. \n",
    "\\begin{equation*}\n",
    "\\mathcal{J}_1'(\\lambda;\\mathcal{D}) =  - \\frac{1}{\\lambda} +  \\bar{y}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4.1) Compute $\\mathcal{J}_1'(\\lambda,\\mathcal{D})$\n",
    "\n",
    "Write a function that computes $\\mathcal{J}_1'(\\lambda,\\mathcal{D})$ for a given $\\lambda$ and $\\mathcal{D}$. (This can be done with one line of code.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.114637Z",
     "iopub.status.busy": "2023-09-14T03:44:13.114313Z",
     "iopub.status.idle": "2023-09-14T03:44:13.121171Z",
     "shell.execute_reply": "2023-09-14T03:44:13.120351Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def curlyJ1prime(lmbda,D):\n",
    "    return ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4.2) Plot $\\mathcal{J}_1'$\n",
    "\n",
    "Plot the gradient $\\mathcal{J}'_1$ evaluated on logarithmically spaced points (numpy `logspace`) ranging from $10^{-2}$ to $10^{-0.5}$. Place a vertical line (matplotlib `axvline`) at $\\lambda_*$.\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"I_2_3_2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.144187Z",
     "iopub.status.busy": "2023-09-14T03:44:13.143876Z",
     "iopub.status.idle": "2023-09-14T03:44:13.360935Z",
     "shell.execute_reply": "2023-09-14T03:44:13.359717Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmbdas = np.logspace(-2,-0.5)\n",
    "J1ps = ...  # TODO\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(...,...,'.-')  # TODO\n",
    "ax.axvline(...,color='r',linestyle='--')  # TODO\n",
    "ax.grid(linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4.3) Code gradient descent\n",
    "\n",
    "Write a function called `GD_1D` that takes arguments \n",
    "+ `Jprime` ... a function that evaluates the derivative of the cost function\n",
    "+ `D` ... the dataset\n",
    "+ `lambda_init` ... the initial condition for gradient descent\n",
    "\n",
    "Use $\\gamma=0.01$ and $K=10$.\n",
    "The function should return a numpy array of length $K$, with `lambda_init` in the zeroth position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.365789Z",
     "iopub.status.busy": "2023-09-14T03:44:13.365472Z",
     "iopub.status.idle": "2023-09-14T03:44:13.374498Z",
     "shell.execute_reply": "2023-09-14T03:44:13.373422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GD_1D(Jprime,D,lmbda_init=0.3):\n",
    "    gamma=0.01\n",
    "    K=10\n",
    "    lmbdas = np.empty(K)\n",
    "    lmbdas[0] = ... # TODO\n",
    "    ...  # TODO (a for loop)\n",
    "    return lmbdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_4_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4.4) Plot gradient descent\n",
    "\n",
    "Create a plot showing the convergence of $\\{\\lambda_k\\}_K$ (from gradient descent) to $\\lambda_*$\n",
    "\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"I_2_3_4.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.391524Z",
     "iopub.status.busy": "2023-09-14T03:44:13.391225Z",
     "iopub.status.idle": "2023-09-14T03:44:13.588501Z",
     "shell.execute_reply": "2023-09-14T03:44:13.587615Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmbdas_gd = GD_1D(curlyJ1prime,D)\n",
    "K_gd = len(lmbdas_gd)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(...,...,'-o')  # TODO\n",
    "ax.hlines(eval_lambda_star(D),0,K_gd,color='r',linestyles=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4.5) Count evaluations of $\\mathcal{J}_1'(\\lambda;\\mathcal{D})$\n",
    "\n",
    "\n",
    "**Hint**: `np.where`\n",
    "\n",
    "How many evaluations of $\\mathcal{J}_1'(\\lambda;\\mathcal{D})$ were needed for gradient descent to reach within 0.01 of $\\lambda^*$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.592953Z",
     "iopub.status.busy": "2023-09-14T03:44:13.592655Z",
     "iopub.status.idle": "2023-09-14T03:44:13.599658Z",
     "shell.execute_reply": "2023-09-14T03:44:13.598681Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_J1prime_evals(lmbdas_gd,lambda_star):\n",
    "    ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_4_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-5) Grid search\n",
    "\n",
    "Grid search is a simple method that does not require gradient information. Also called *exhaustive search*, the idea is simply to grid the search space, evaluate the cost function on every grid point, and choose the best one. This is obviously an inefficient thing to do, but it is sometimes expedient if the search space is realtively small, evaluating the cost function is relatively cheap, and there is no gradient information available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-5.1) Code grid search\n",
    "\n",
    "Code grid search for a uniform grid on the interval $[0,1]$. The arguments to the function are \n",
    "+ `J` ... the cost function\n",
    "+ `D` ... the dataset \n",
    "+ `gridstep` ... the distance between points on the grid.\n",
    "\n",
    "This function should return \n",
    "+ `lmbdas` ... the array of grid points\n",
    "+ `Js` ... the cost function evaluated on this array\n",
    "+ `ind` ... the index of the optimal grid point.\n",
    "\n",
    "Use this function to find a solution that is within 0.01 from  $\\lambda_*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.614470Z",
     "iopub.status.busy": "2023-09-14T03:44:13.614109Z",
     "iopub.status.idle": "2023-09-14T03:44:13.621634Z",
     "shell.execute_reply": "2023-09-14T03:44:13.620777Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_search(J,D,gridstep):\n",
    "   lmbdas = np.arange(...)  # TODO\n",
    "   Js = ...  # TODO\n",
    "   ind = Js.argmin()\n",
    "   return lmbdas, Js, ind\n",
    "\n",
    "lmbda_gs, Js_gs, ind_gs = grid_search(curlyJ1,D,gridstep=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_5_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-5.2) Count evaluations of $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ \n",
    "\n",
    "How many evaluations of $\\mathcal{J}_1(\\lambda;\\mathcal{D})$ were needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.638049Z",
     "iopub.status.busy": "2023-09-14T03:44:13.637400Z",
     "iopub.status.idle": "2023-09-14T03:44:13.644528Z",
     "shell.execute_reply": "2023-09-14T03:44:13.643395Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_J1_evals(lmbda_gs):\n",
    "    return ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"I_5_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-5.3) Plot grid search\n",
    "\n",
    "Plot `lmbda_gs` versus `Js_gs`. Indicate the best grid point as well as $\\lambda_*$ with vertical lines.  \n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"I_2_4_3.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.659289Z",
     "iopub.status.busy": "2023-09-14T03:44:13.658638Z",
     "iopub.status.idle": "2023-09-14T03:44:13.916731Z",
     "shell.execute_reply": "2023-09-14T03:44:13.915862Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.plot(...,...,'.-')  # TODO\n",
    "ax.axvline(...,color='r')  # TODO\n",
    "ax.axvline(...,color='m')  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part II:** Estimating $\\mu$ and $\\sigma^2$ for a normal distribution\n",
    "\n",
    "---\n",
    "\n",
    "We now repeat the exercise with a different system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.921048Z",
     "iopub.status.busy": "2023-09-14T03:44:13.920725Z",
     "iopub.status.idle": "2023-09-14T03:44:13.926769Z",
     "shell.execute_reply": "2023-09-14T03:44:13.925911Z"
    }
   },
   "outputs": [],
   "source": [
    "D = draw_50_samples_from_II()\n",
    "N = D.shape[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a scatter plot and histogram of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:13.931206Z",
     "iopub.status.busy": "2023-09-14T03:44:13.930552Z",
     "iopub.status.idle": "2023-09-14T03:44:14.407981Z",
     "shell.execute_reply": "2023-09-14T03:44:14.406961Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5),nrows=2,sharex=True)\n",
    "ax[0].plot(D,np.zeros(N),'r.')\n",
    "ax[1].hist(D);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot clearly shows that the data cannot be exponentially distributed: it includes negative values, and the histogram is bell-shaped. We decide that a Gaussian distribution provides the best fit.  The pdf for the Gaussian distribution is:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(y;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left( -\\frac{(y-\\mu)^2}{2\\sigma^2}   \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Plugging this into the maximum likelihood cost function and doing some simple algebra we obtain:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{J}_2(\\mu,\\sigma^2;\\mathcal{D}) = \n",
    "\\frac{1}{2}\\ln(2\\pi\\sigma^2)\n",
    "+\\frac{1}{2N\\sigma^2}\\sum_{i=1}^N (y_i-\\mu)^2 \n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II-1) Cost function\n",
    "\n",
    "Write a function that computes $\\mathcal{J}_2(\\mu,\\sigma^2;\\mathcal{D})$ for given $\\mu$, $\\sigma^2$, and dataset $\\mathcal{D}$. (This can be done in one line of code.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:14.412177Z",
     "iopub.status.busy": "2023-09-14T03:44:14.411858Z",
     "iopub.status.idle": "2023-09-14T03:44:14.419199Z",
     "shell.execute_reply": "2023-09-14T03:44:14.418345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def curlyJ2(mu,sigma2,D):\n",
    "    return ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-2) Analytical solution\n",
    "\n",
    "We can use algebra to find a solution, just as we did with the exponential distribution, by equating the derivative of the cost function to zero. Now however, because our search space is two-dimensional $(\\mu,\\sigma^2)$, we must work with a vector *gradient* instead of a scalar *derivative*. \n",
    "\n",
    "You can verify that the partial derivatives of $\\mathcal{J}_2(\\mu,\\sigma^2;\\mathcal{D})$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu} &= \\frac{\\mu-\\bar{y}}{\\sigma^2} \\\\ \n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2} &= \\frac{1}{2\\sigma^2} \\left(1 -\t\n",
    "\\frac{1}{N\\sigma^2}\\sum_{i=1}^N \\left( y_i-\\mu  \\right)^2 \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Equating $\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu}$ and $\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2}$ to zero we get two equations for stationary $\\mu$ and $\\sigma^2$. From these we can find the *unique* stationary point, which is also the solution to the optimization problem:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_* &= \\bar{y} \\\\\n",
    "\\sigma^2_* &= \\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\bar{y})^2\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2.1) Compute $\\mu_*$ and $\\sigma^2_*$\n",
    "\n",
    "Write a function that computes $\\mu_*$ and $\\sigma^2_*$ for a given $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:14.433135Z",
     "iopub.status.busy": "2023-09-14T03:44:14.432827Z",
     "iopub.status.idle": "2023-09-14T03:44:14.439528Z",
     "shell.execute_reply": "2023-09-14T03:44:14.438746Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_mu_sigma2_star(D):\n",
    "    mu_star = ...   # TODO\n",
    "    sigma2_star = ...   # TODO\n",
    "    return mu_star, sigma2_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-3) Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3.1) Compute the gradient\n",
    "\n",
    "Write a function that computes the gradient $\\left(\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu},\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2}\\right)$ as a function of $\\mu$, $\\sigma^2$, and $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:14.453874Z",
     "iopub.status.busy": "2023-09-14T03:44:14.453232Z",
     "iopub.status.idle": "2023-09-14T03:44:14.460606Z",
     "shell.execute_reply": "2023-09-14T03:44:14.459838Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient(mu,sigma2,D):\n",
    "    N = ...   # TODO\n",
    "    partialMu = ...   # TODO\n",
    "    partialSigma2 = ...   # TODO\n",
    "    return partialMu, partialSigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3.2) 2D gradient descent\n",
    "\n",
    "Write the gradient descent algorithm for this 2D case. The code should be very similar to the 1D function.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:14.475247Z",
     "iopub.status.busy": "2023-09-14T03:44:14.474928Z",
     "iopub.status.idle": "2023-09-14T03:44:14.484059Z",
     "shell.execute_reply": "2023-09-14T03:44:14.483113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GD_2D(GRAD,D,mu_init,sigma2_init,maxsteps=100):\n",
    "    gamma_mu, gamma_sigma2 = 1.5, 20\n",
    "    mu = np.empty(maxsteps)\n",
    "    mu[0] = ...   # TODO\n",
    "    sigma2 = np.empty(maxsteps)\n",
    "    sigma2[0] = ...   # TODO\n",
    "    for k in range(...):   # TODO\n",
    "        delmu, delsigma2 = GRAD(mu[k],sigma2[k],D)\n",
    "        mu[k+1] = ...   # TODO\n",
    "        sigma2[k+1] = ...   # TODO\n",
    "    return mu, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_3_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3.3) Plot gradient descent\n",
    "\n",
    "Your plot should look like this:\n",
    "\n",
    "<img src=\"II_2_3_3.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:14.511687Z",
     "iopub.status.busy": "2023-09-14T03:44:14.511397Z",
     "iopub.status.idle": "2023-09-14T03:44:14.714720Z",
     "shell.execute_reply": "2023-09-14T03:44:14.713875Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu_gd, sigma2_gd = GD_2D(gradient,D,mu_init=0.5,sigma2_init=10)\n",
    "mu_star, sigma2_star = eval_mu_sigma2_star(D)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(...,...,'.-')   # TODO\n",
    "ax.plot(...,...,'r*',markersize=16)   # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3.4) Run this GD over a grid of initial conditions.\n",
    "\n",
    "The next cell runs gradient descent over a grid of initial conditions in the $(\\mu,\\sigma^2)$ plane. Notice that in every case the trajectory converges toward the true solution of the optimization problem $(\\mu_*,\\sigma^2_*)$, shown as a red star in the first plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:14.718792Z",
     "iopub.status.busy": "2023-09-14T03:44:14.718491Z",
     "iopub.status.idle": "2023-09-14T03:44:15.916377Z",
     "shell.execute_reply": "2023-09-14T03:44:15.914970Z"
    }
   },
   "outputs": [],
   "source": [
    "mu_star, sigma2_star = eval_mu_sigma2_star(D)\n",
    "\n",
    "mu_range = np.linspace(0.1,2,10)\n",
    "sigma2_range = np.arange(15,25)\n",
    "\n",
    "_, ax1 = plt.subplots()\n",
    "ax1.set_ylabel('sigma2')\n",
    "ax1.set_xlabel('mu')\n",
    "\n",
    "_, ax2 = plt.subplots(nrows=2,sharex=True)\n",
    "ax2[0].set_ylabel('mu')\n",
    "ax2[1].set_ylabel('sigma2')\n",
    "ax2[1].set_xlabel('k')\n",
    "\n",
    "for i, mu_init in enumerate(mu_range):\n",
    "    for j, sigma2_init in enumerate(sigma2_range):\n",
    "        mu_gd, sigma2_gd = GD_2D(gradient,D,mu_init=mu_init,sigma2_init=sigma2_init)\n",
    "        ax1.plot(mu_gd,sigma2_gd)\n",
    "        ax2[0].plot(mu_gd)\n",
    "        ax2[1].plot(sigma2_gd)\n",
    "\n",
    "ax1.plot(mu_star,sigma2_star,'r*',markersize=16)\n",
    "ax2[0].axhline(mu_star,0,100,color='r',linestyle='--')\n",
    "ax2[1].axhline(sigma2_star,0,100,color='r',linestyle='--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-4) Stochastic gradient descent\n",
    "\n",
    "Recall the formula for the partial derivatives of $\\mathcal{J}_2$. It can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu} &= \\frac{1}{\\sigma^2} \\left( \\mu - \\frac{1}{N}\\sum_{y_i\\in\\mathcal{D}} y_i \\right) \\\\ \n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2} &= \\frac{1}{2\\sigma^2} \\left(1 -\t\n",
    "\\frac{1}{\\sigma^2}\\frac{1}{N}\\sum_{y_i\\in\\mathcal{D}} \\left( y_i-\\mu  \\right)^2 \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Notice that both of these involve a sum over the dataset $\\mathcal{D}$. While the first one can be pre-computed, the second is inextricable linked to $\\mu$, and must therefore be re-computed at each step of gradient descent. This can be very time-consuming when $N$ is large. \n",
    "\n",
    "Stochastic gradient descent is the simple idea of approximating the gradient based on a sub-sample of the dataset $\\mathcal{D}$, which we call a **batch** $\\mathcal{B}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\mu} &\\approx \\frac{1}{\\sigma^2} \\left( \\mu - \\frac{1}{|\\mathcal{B}|}\\sum_{y_i\\in\\mathcal{B}} y_i \\right) \\\\ \n",
    "\\frac{\\partial \\mathcal{J}_2}{\\partial \\sigma^2} &\\approx \\frac{1}{2\\sigma^2} \\left(1 -\t\n",
    "\\frac{1}{\\sigma^2}\\frac{1}{|\\mathcal{B}|}\\sum_{y_i\\in\\mathcal{B}} \\left( y_i-\\mu  \\right)^2 \\right)\n",
    "\\end{align*}\n",
    "Here $|\\mathcal{B}|$ is the number of samples in the batch.\n",
    "Batches are obtained by splitting the dataset into equal parts. We will test SGD with batches of size 5. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-4.1) Code stochastic gradient descent\n",
    "\n",
    "Next we will code stochastic gradient descent. The code is very similar to `GD_2D` with a few exceptions to note:\n",
    "\n",
    "1) It should begin by splitting $\\mathcal{D}$ into batches of size `batch_size`. The code below uses the `reshape` method to do this. Understand how it works, and then you can copy it directly into your `SGD` method.\n",
    "\n",
    "``` python\n",
    "batch_size = 5\n",
    "N = D.shape[0]\n",
    "num_batches = int(N/batch_size)\n",
    "batches = D.reshape((num_batches,batch_size))\n",
    "```\n",
    "\n",
    "2) Each step in the iteration is based on a new batch. After reaching the last batch, you should begin again with the first. In other words, at the `k`'th step you should use the `(k % num_batches)`'th batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:15.921247Z",
     "iopub.status.busy": "2023-09-14T03:44:15.920877Z",
     "iopub.status.idle": "2023-09-14T03:44:15.932936Z",
     "shell.execute_reply": "2023-09-14T03:44:15.931715Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SGD(GRAD,D,batch_size,mu_init,sigma2_init,maxsteps=100):\n",
    "\n",
    "    # Split D into batches (see item 1 above)\n",
    "    ...  # TODO\n",
    "\n",
    "    # Initialization (same as GD_2D)\n",
    "    gamma_mu, gamma_sigma2 = 1,20\n",
    "    mu = np.empty(maxsteps)\n",
    "    mu[0] = mu_init\n",
    "    sigma2 = np.empty(maxsteps)\n",
    "    sigma2[0] = sigma2_init\n",
    "\n",
    "    for k in range(maxsteps-1):\n",
    "\n",
    "        # Choose the batch (see item 2 above)\n",
    "        B = ...   # TODO\n",
    "\n",
    "        # Same as GD_2D\n",
    "        delmu, delsigma2 = ...  # TODO\n",
    "        mu[k+1] = ...  # TODO\n",
    "        sigma2[k+1] = ...  # TODO\n",
    "        \n",
    "    return mu, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"II_4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-4.2) Plot stochastic gradient descent\n",
    "\n",
    "The next cell repeats the plots we made for gradient descent, but now using stochastic gradient descent. Notice that the trajectories are much more jagged, and they do not actually converge to the true solution. In this sense, stochastic gradient descent is *not* a good algorithm for solving optimization problems. However it is popular in machine learning applications because a) it is faster than GD for large datasets, and b) approximate convergence (as opposed to exact convergence) is often good enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T03:44:15.966715Z",
     "iopub.status.busy": "2023-09-14T03:44:15.966157Z",
     "iopub.status.idle": "2023-09-14T03:44:17.461713Z",
     "shell.execute_reply": "2023-09-14T03:44:17.460558Z"
    }
   },
   "outputs": [],
   "source": [
    "mu_range = np.linspace(0.1,2,10)\n",
    "sigma2_range = np.arange(15,25)\n",
    "batch_size = 5\n",
    "\n",
    "_, ax1 = plt.subplots()\n",
    "ax1.set_ylabel('sigma2')\n",
    "ax1.set_xlabel('mu')\n",
    "\n",
    "_, ax2 = plt.subplots(nrows=2,sharex=True)\n",
    "ax2[0].set_ylabel('mu')\n",
    "ax2[1].set_ylabel('sigma2')\n",
    "ax2[1].set_xlabel('k')\n",
    "\n",
    "for i, mu_init in enumerate(mu_range):\n",
    "    for j, sigma2_init in enumerate(sigma2_range):\n",
    "        mu_gd, sigma2_gd = SGD(gradient,D,batch_size,mu_init=mu_init,sigma2_init=sigma2_init)\n",
    "        ax1.plot(mu_gd,sigma2_gd,linewidth=0.5)\n",
    "        ax2[0].plot(mu_gd)\n",
    "        ax2[1].plot(sigma2_gd)\n",
    "\n",
    "ax1.plot(mu_star,sigma2_star,'r*',markersize=16)\n",
    "ax2[0].axhline(mu_star,0,100,color='r',linestyle='--')\n",
    "ax2[1].axhline(sigma2_star,0,100,color='r',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "II_1": {
     "name": "II_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(curlyJ2(1,3,D),4.617293863079248,0.01)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "II_2_1": {
     "name": "II_2_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> mu_star, sigma2_star = eval_mu_sigma2_star(D)\n>>> np.isclose(mu_star,0.7896621525759467,0.01) and np.isclose(sigma2_star,18.850053103184145,0.01)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "II_3_1": {
     "name": "II_3_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> dmu,dsigma2 = gradient(2,2,D)\n>>> np.isclose(dmu,0.6051689237120266,0.01) and np.isclose(dsigma2,-2.2893713510114044,0.01)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "II_3_2": {
     "name": "II_3_2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> mu_gd, sigma2_gd = GD_2D(gradient,D,mu_init=0.5,sigma2_init=10)\n>>> \n>>> np.all(np.isclose(mu_gd[::10],np.array([0.5       , 0.71019491, 0.76155619, 0.77886961, 0.785336  ,\n...        0.7878823 , 0.78891733, 0.78934683, 0.78952756, 0.78960437]),0.01)) \nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> np.all(np.isclose(sigma2_gd[::10],np.array([10.        , 14.40685386, 15.99862629, 16.89510573, 17.46572469,\n...        17.8511153 , 18.12048071, 18.31287942, 18.45230824, 18.55436728]),0.01))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "II_4_1": {
     "name": "II_4_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> mu_gd, sigma2_gd = SGD(gradient,D,batch_size=5,mu_init=0.5,sigma2_init=10)\n>>> np.all(np.isclose(mu_gd[::10],np.array([[0.5       , 0.67550016, 0.73897083, 0.76956495, 0.78546419,\n...        0.79403962, 0.79876485, 0.80140247, 0.80288618, 0.80372414]]),0.01))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> mu_gd, sigma2_gd = SGD(gradient,D,batch_size=5,mu_init=0.5,sigma2_init=10)\n>>> np.all(np.isclose(sigma2_gd[::10],np.array([10.        , 14.52311796, 16.0973107 , 16.98220092, 17.54535584,\n...        17.92577878, 18.19173331, 18.38173057, 18.51943563, 18.6202391 ]),0.01))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_2": {
     "name": "I_2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(curlyJ1(1,D),7.126381758478155,0.01)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(curlyJ1(10,D),68.9612324917875,0.01)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_3_1": {
     "name": "I_3_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(eval_lambda_star(D),0.1403236640824517,0.01)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_3_3_1": {
     "name": "I_3_3_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.all(np.isclose(J1_samples[0,:10],np.array([4.70367627, 4.64038296, 4.57761504, 4.51541087, 4.45381162,\n...                             4.39286148, 4.33260785, 4.27310158, 4.21439728, 4.1565535 ]),1e-4))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.all(np.isclose(J1_samples[50:60,30],np.array([3.39518542, 3.32719512, 3.23129694, 3.49043681, 3.31114189,\n...        3.31881383, 3.12304334, 3.28283946, 3.07741875, 3.34139624]),1e-4))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.all(np.isclose(J1_samples[-10:,-1],np.array([4.81254377, 4.39403784, 3.72344147, 4.26690454, 3.96288446,\n...        4.42441761, 4.28726234, 4.54233622, 4.68807586, 4.04252632]),1e-4))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_4_1": {
     "name": "I_4_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(curlyJ1prime(1,D),8.14288396070683,0.01)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(curlyJ1prime(10,D),9.042883960706831,0.01)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_4_3": {
     "name": "I_4_3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> lmbdas_gd = GD_1D(curlyJ1prime,D)\n>>> np.all(np.isclose(lmbdas_gd,np.array([0.3 , 0.24190449, 0.19181428, 0.1525192 , 0.12665588,\n...        0.11418113, 0.11033244, 0.10953877, 0.10940181, 0.10937914]),0.01))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_4_5": {
     "name": "I_4_5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> count_J1prime_evals(lmbdas_gd,eval_lambda_star(D))==5\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_5_1": {
     "name": "I_5_1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.all(np.isclose(lmbda_gs,np.array([0.  , 0.02, 0.04, 0.06, 0.08, 0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 ,\n...        0.22, 0.24, 0.26, 0.28, 0.3 , 0.32, 0.34, 0.36, 0.38, 0.4 , 0.42,\n...        0.44, 0.46, 0.48, 0.5 , 0.52, 0.54, 0.56, 0.58, 0.6 , 0.62, 0.64,\n...        0.66, 0.68, 0.7 , 0.72, 0.74, 0.76, 0.78, 0.8 , 0.82, 0.84, 0.86,\n...        0.88, 0.9 , 0.92, 0.94, 0.96, 0.98]),0.01))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "I_5_2": {
     "name": "I_5_2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> count_J1_evals(lmbda_gs)==50\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
